{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2017741046_임준오_Assignment_10_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhsTCklOvV98"
      },
      "source": [
        "##**2 Assignment**\n",
        "\n",
        "###**다음과 같은 CNN 모델을 작성해보자**\n",
        "* Input\n",
        "  * Input type: torch.Tensor\n",
        "  * Input shape: (?, 1, 28, 28)\n",
        "    * 여러장의, 흑백, 28x28 size의 이미지라고 가정하자\n",
        "* Layers\n",
        "  * Layer1\n",
        "    * Conv2d >> C: 32, Kernel size (필터 크기): 3, Stride: 1, Padding: 1\n",
        "    * ReLU\n",
        "    * MaxPool >> Kernel size: 2, Stride: 2\n",
        "    * 입-출력 (?, 1, 28, 28) >> (?, 32, 14, 14)\n",
        "  * Layer2\n",
        "    * Conv2d >> C: 64, Kernel size (필터 크기): 3, Stride: 1, Padding: 1\n",
        "    * ReLU\n",
        "    * MaxPool >> Kernel size: 2, Stride: 2\n",
        "    * 입-출력 (?, 32, 14, 14) >> (?, 64, 7, 7)\n",
        "  * Layer3\n",
        "    * Conv2d >> C: 128, Kernel size (필터 크기): 3, Stride: 1, Padding: 1\n",
        "    * ReLU\n",
        "    * MaxPool >> Kernel size: 2, Stride: 2, Padding: 1\n",
        "    * 입-출력 (?, 64, 7, 7) >> (?, 128, 4, 4)\n",
        "  * Layer4\n",
        "    * Linear >> input: 4x4x128 output: 625\n",
        "    * ReLU\n",
        "    * Dropout\n",
        "    * 입-출력 (4x4x128) >> (625)\n",
        "  * Layer5\n",
        "    * Linear >> input: 625 output: 10\n",
        "    * Softmax (pytorch의 Cross Entropy Loss 함수를 사용하는 것을 감안한다)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6pWazEjvPpf",
        "outputId": "57ae6e67-23c1-442f-b054-fe4e13850c66"
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn.functional as func\n",
        "import torch.optim as opt\n",
        "from torchvision import datasets, transforms\n",
        "import pandas as pd\n",
        "\n",
        "batch_size = 64\n",
        "train_data = datasets.MNIST(root='./data/', train=True,\n",
        "                               transform=transforms.ToTensor(), \n",
        "                               download=True)\n",
        "test_data = datasets.MNIST(root='./data/', train=False,\n",
        "                               transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_data,\n",
        "                                           batch_size=batch_size, \n",
        "                                           shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_data,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model,self).__init__()\n",
        "        #동일한 방법\n",
        "        '''self.layer1 = nn.Sequential(nn.Conv2d(1, 32, 3, padding=1),\n",
        "                      nn.ReLU(), nn.MaxPool2d(2, stride=2))\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(32, 64, 3, padding=1),\n",
        "                      nn.ReLU(), nn.MaxPool2d(2, stride=2))\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1),\n",
        "                      nn.ReLU(), nn.MaxPool2d(2, stride=2, padding=1))\n",
        "        self.layer4 = nn.Sequential(nn.Linear(4*4*128, 625),\n",
        "                      nn.ReLU(), nn.Dropout())\n",
        "        self.linear = nn.Linear(625, 10)'''\n",
        "\n",
        "        #동일한 방법\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.mp1 = nn.MaxPool2d(2, stride=2)\n",
        "        self.mp2 = nn.MaxPool2d(2, stride=2, padding=1)\n",
        "        self.linear1 = nn.Linear(4*4*128, 625)\n",
        "        self.linear2 = nn.Linear(625, 10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        in_size = x.size(0)\n",
        "        #동일한 방법\n",
        "        '''x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = x.reshape(in_size, -1)\n",
        "        x = self.layer4(x)\n",
        "        x = self.linear(x) #layer5'''\n",
        "\n",
        "        #동일한 방법\n",
        "        x = func.relu(self.mp1(self.conv1(x))) #layer1\n",
        "        x = func.relu(self.mp1(self.conv2(x))) #layer2\n",
        "        x = func.relu(self.mp2(self.conv3(x))) #layer3\n",
        "        x = x.reshape(in_size, -1)\n",
        "        x = func.dropout(func.relu(self.linear1(x))) #layer4\n",
        "        x = self.linear2(x) #layer5\n",
        "        return x\n",
        "\n",
        "model = Model()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = opt.SGD(model.parameters(),lr=0.2, momentum=0.5)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train() \n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data) \n",
        "        loss = criterion(output, target) \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0: \n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]  Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test():\n",
        "    model.eval() \n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        output = model(data) \n",
        "        test_loss = criterion(output, target)\n",
        "        pred = output.data.max(1, keepdim=True)[1] \n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "        \n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset), #accuracy 출력\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "    \n",
        "for i in range(5):\n",
        "    train(i)\n",
        "test()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/60000 (0%)]  Loss: 2.298148\n",
            "Train Epoch: 0 [6400/60000 (11%)]  Loss: 0.267909\n",
            "Train Epoch: 0 [12800/60000 (21%)]  Loss: 0.079454\n",
            "Train Epoch: 0 [19200/60000 (32%)]  Loss: 0.072545\n",
            "Train Epoch: 0 [25600/60000 (43%)]  Loss: 0.310253\n",
            "Train Epoch: 0 [32000/60000 (53%)]  Loss: 0.152130\n",
            "Train Epoch: 0 [38400/60000 (64%)]  Loss: 0.091215\n",
            "Train Epoch: 0 [44800/60000 (75%)]  Loss: 0.124655\n",
            "Train Epoch: 0 [51200/60000 (85%)]  Loss: 0.072367\n",
            "Train Epoch: 0 [57600/60000 (96%)]  Loss: 0.005575\n",
            "Train Epoch: 1 [0/60000 (0%)]  Loss: 0.059271\n",
            "Train Epoch: 1 [6400/60000 (11%)]  Loss: 0.040458\n",
            "Train Epoch: 1 [12800/60000 (21%)]  Loss: 0.018803\n",
            "Train Epoch: 1 [19200/60000 (32%)]  Loss: 0.009881\n",
            "Train Epoch: 1 [25600/60000 (43%)]  Loss: 0.139022\n",
            "Train Epoch: 1 [32000/60000 (53%)]  Loss: 0.117092\n",
            "Train Epoch: 1 [38400/60000 (64%)]  Loss: 0.095115\n",
            "Train Epoch: 1 [44800/60000 (75%)]  Loss: 0.009084\n",
            "Train Epoch: 1 [51200/60000 (85%)]  Loss: 0.043295\n",
            "Train Epoch: 1 [57600/60000 (96%)]  Loss: 0.009833\n",
            "Train Epoch: 2 [0/60000 (0%)]  Loss: 0.024177\n",
            "Train Epoch: 2 [6400/60000 (11%)]  Loss: 0.003833\n",
            "Train Epoch: 2 [12800/60000 (21%)]  Loss: 0.036919\n",
            "Train Epoch: 2 [19200/60000 (32%)]  Loss: 0.063249\n",
            "Train Epoch: 2 [25600/60000 (43%)]  Loss: 0.011654\n",
            "Train Epoch: 2 [32000/60000 (53%)]  Loss: 0.029944\n",
            "Train Epoch: 2 [38400/60000 (64%)]  Loss: 0.018103\n",
            "Train Epoch: 2 [44800/60000 (75%)]  Loss: 0.031364\n",
            "Train Epoch: 2 [51200/60000 (85%)]  Loss: 0.004534\n",
            "Train Epoch: 2 [57600/60000 (96%)]  Loss: 0.077961\n",
            "Train Epoch: 3 [0/60000 (0%)]  Loss: 0.023078\n",
            "Train Epoch: 3 [6400/60000 (11%)]  Loss: 0.014171\n",
            "Train Epoch: 3 [12800/60000 (21%)]  Loss: 0.013142\n",
            "Train Epoch: 3 [19200/60000 (32%)]  Loss: 0.002935\n",
            "Train Epoch: 3 [25600/60000 (43%)]  Loss: 0.017992\n",
            "Train Epoch: 3 [32000/60000 (53%)]  Loss: 0.061394\n",
            "Train Epoch: 3 [38400/60000 (64%)]  Loss: 0.009534\n",
            "Train Epoch: 3 [44800/60000 (75%)]  Loss: 0.047143\n",
            "Train Epoch: 3 [51200/60000 (85%)]  Loss: 0.121793\n",
            "Train Epoch: 3 [57600/60000 (96%)]  Loss: 0.021150\n",
            "Train Epoch: 4 [0/60000 (0%)]  Loss: 0.004473\n",
            "Train Epoch: 4 [6400/60000 (11%)]  Loss: 0.000431\n",
            "Train Epoch: 4 [12800/60000 (21%)]  Loss: 0.006687\n",
            "Train Epoch: 4 [19200/60000 (32%)]  Loss: 0.040356\n",
            "Train Epoch: 4 [25600/60000 (43%)]  Loss: 0.002528\n",
            "Train Epoch: 4 [32000/60000 (53%)]  Loss: 0.000130\n",
            "Train Epoch: 4 [38400/60000 (64%)]  Loss: 0.017946\n",
            "Train Epoch: 4 [44800/60000 (75%)]  Loss: 0.003384\n",
            "Train Epoch: 4 [51200/60000 (85%)]  Loss: 0.019508\n",
            "Train Epoch: 4 [57600/60000 (96%)]  Loss: 0.007055\n",
            "\n",
            "Test set: Average loss: 0.0002, Accuracy: 9869/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    }
  ]
}