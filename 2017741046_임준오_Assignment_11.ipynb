{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2017741046_임준오_Assignment_11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plyATQM4MVu8"
      },
      "source": [
        "##**3 Assignment**\n",
        "\n",
        "###**다음 미완성 코드를 활용해 새로운 data를 학습해보자**\n",
        "\n",
        "* Training data image\n",
        "  * Shape: (?, 3, 64, 128)\n",
        "    * 여러장의, RGB, 64x128 size의 이미지라고 가정하자\n",
        "\n",
        "* Test data image\n",
        "  * Shape: (?, 3, ?, ?)\n",
        "    * 여러장의, RGB, size를 알 수 없는 이미지라고 가정하자\n",
        "\n",
        "* Labels\n",
        "  * image의 class는 2가지 이다\n",
        "  * data folder의 구성을 참고하자\n",
        "\n",
        "* Data folder 위치\n",
        "  * 노트북 github의 data folder 2개를 적당한 위치에 카피해 사용한다"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvVjhzH6e0uZ",
        "outputId": "f2b106e1-8f72-47a9-c633-8f368a8e169e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "import torchvision.datasets as dsets\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as opt\n",
        "from torch.utils.data import DataLoader\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "tf = transforms.Compose([transforms.Resize((64,128)), transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "trainset = dsets.ImageFolder(root = '/content/drive/MyDrive/origin_data',transform=tf)\n",
        "testset = dsets.ImageFolder(root = '/content/drive/MyDrive/test_data',transform=tf)\n",
        "\n",
        "trainloader = DataLoader(trainset, batch_size=16, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=16, shuffle=True)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN,self).__init__()\n",
        "        self.layer1 = nn.Sequential(nn.Conv2d(3, 32, 3, padding=1),\n",
        "                      nn.ReLU(), nn.MaxPool2d(2, stride=2))\n",
        "        self.layer2 = nn.Sequential(nn.Conv2d(32, 64, 3, padding=1),\n",
        "                      nn.ReLU(), nn.MaxPool2d(2, stride=2))\n",
        "        self.layer3 = nn.Sequential(nn.Conv2d(64, 128, 3, padding=1),\n",
        "                      nn.ReLU(), nn.MaxPool2d(2, stride=2, padding=1))\n",
        "        self.layer4 = nn.Sequential(nn.Linear(19584, 5000),\n",
        "                      nn.ReLU(), nn.Dropout()) \n",
        "        self.linear1 = nn.Linear(5000, 500)\n",
        "        self.linear2 = nn.Linear(500, 2) \n",
        "\n",
        "    def forward(self,x):\n",
        "        in_size = x.size(0)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = x.reshape(in_size, -1)\n",
        "        x = self.layer4(x)\n",
        "        x = self.linear1(x)\n",
        "        x = self.linear2(x) \n",
        "        return x\n",
        "\n",
        "learning_rate = 0.004\n",
        "training_epochs = 10\n",
        "\n",
        "model = CNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = opt.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "total_batch = len(trainloader)\n",
        "for epoch in range(training_epochs):\n",
        "    total_loss = 0\n",
        "    for inputs, labels in trainloader:\n",
        "        inputs,labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss / total_batch\n",
        "\n",
        "    print('[Epoch: {:>4}] loss = {:>.9}'.format(epoch + 1, total_loss))\n",
        "\n",
        "with torch.no_grad():\n",
        "    for test_inputs, test_labels in testloader:\n",
        "        test_inputs,test_labels = test_inputs.to(device), test_labels.to(device)\n",
        "        test_output = model(test_inputs)\n",
        "        correct_prediction = torch.argmax(test_output, 1) == test_labels\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy of test images:', accuracy.item() * 100,'%')\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch:    1] loss = 1.64954174\n",
            "[Epoch:    2] loss = 0.289046466\n",
            "[Epoch:    3] loss = 1.0562706\n",
            "[Epoch:    4] loss = 0.271726787\n",
            "[Epoch:    5] loss = 1.80935231e-07\n",
            "[Epoch:    6] loss = 0.0\n",
            "[Epoch:    7] loss = 0.0\n",
            "[Epoch:    8] loss = 0.0\n",
            "[Epoch:    9] loss = 0.0\n",
            "[Epoch:   10] loss = 0.0\n",
            "Accuracy of test images: 100.0 %\n"
          ]
        }
      ]
    }
  ]
}