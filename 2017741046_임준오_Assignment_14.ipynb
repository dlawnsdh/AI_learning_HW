{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2017741046_임준오_Assignment_14.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##**4 Assignment**\n",
        "a) 아래 예제 코드를 이용해 텍스트 입력의 숫자 변환 과정을 체크한다\n",
        "\n",
        "b) testset의 임의 입력을 LongTensor로 변환해 학습 완료된 모델에 입력해보고, 결과가 어떠한지 체크한다 (하단의 출력 결과를 참조)"
      ],
      "metadata": {
        "id": "WlmVO4FGvu8R"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TIX3vNnWvqlQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21d80b36-0f2d-4bc6-ecbd-622126f92c8e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n",
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 84.1M/84.1M [00:02<00:00, 39.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TrainSet]: 20000 [ValSet]: 5000 [TestSet]: 25000 [Vocab]: 46159 [Classes] 2\n",
            "[Epoch:    1] cost = 3.41717672\n",
            "[Epoch:    2] cost = 3.39704394\n",
            "[Epoch:    3] cost = 3.38808107\n",
            "[Epoch:    4] cost = 3.39739299\n",
            "[Epoch:    5] cost = 3.39174867\n",
            "[Epoch:    6] cost = 3.39224935\n",
            "[Epoch:    7] cost = 2.93911386\n",
            "[Epoch:    8] cost = 1.53755748\n",
            "[Epoch:    9] cost = 0.798684776\n",
            "[Epoch:   10] cost = 0.374675035\n",
            "accuracy =  tensor(86.9600, device='cuda:0')\n",
            "accuracy =  tensor(431.2000, device='cuda:0')\n",
            "['i', 'am', 'a', '20', 'year', 'old', 'bloke', 'from', 'england.', 'i', \"don't\", 'cry', 'at', 'films.', 'but', 'this', 'one', 'moved', 'me.', 'i', 'cried', 'like', 'a', 'girl!', 'this', 'is', 'absolutely', 'the', 'most', 'moving', 'film', 'i', 'have', 'ever', 'seen,', 'even', 'though', 'the', 'story', 'was', 'questionable.', 'joseph', \"mazello's\", 'little', 'face', 'when', 'he', 'dreams', 'of', 'crying', 'made', 'me', 'sob', 'every', 'time.', 'how', 'could', 'anyone', 'hurt', 'such', 'a', 'sweet', 'looking', 'kid?', \"i'm\", 'going', 'to', 'cry', 'now', 'just', 'thinking', 'about', 'it!', 'remarkable.']\n",
            "9, 226, 3, 985, 383, 170, 11122, 34, 7890, 9, 86, 1794, 29, 723, 18, 10, 30, 1657, 472, 9, 4990, 37, 3, 22808, 10, 7, 371, 2, 78, 867, 24, 9, 26, 125, 1611, 55, 192, 2, 75, 14, 20673, 2393, 0, 105, 469, 50, 27, 1877, 5, 2948, 95, 87, 25086, 158, 293, 79, 90, 259, 1790, 130, 3, 1365, 275, 43429, 145, 162, 6, 1794, 204, 39, 546, 43, 1045, 11559,  \n",
            "----------------------\n",
            "tensor([1], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchtext.legacy import data, datasets \n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(device)\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)\n",
        "\n",
        "# parameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "training_epochs = 10\n",
        "\n",
        "TEXT = data.Field(sequential=True, batch_first=True, lower=True)\n",
        "LABEL = data.Field(sequential=False, batch_first=True)\n",
        "trainset, testset = datasets.IMDB.splits(TEXT, LABEL)\n",
        "\n",
        "TEXT.build_vocab(trainset, min_freq=5)\n",
        "LABEL.build_vocab(trainset)\n",
        "\n",
        "trainset, valset = trainset.split(split_ratio=0.8)\n",
        "train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
        "        (trainset, valset, testset), batch_size=batch_size,\n",
        "        shuffle=True, repeat=False)\n",
        "\n",
        "vocab_size = len(TEXT.vocab)\n",
        "n_classes = 2 \n",
        "\n",
        "print(\"[TrainSet]: %d [ValSet]: %d [TestSet]: %d [Vocab]: %d [Classes] %d\"\n",
        "      % (len(trainset),len(valset), len(testset), vocab_size, n_classes))\n",
        "\n",
        "class BasicGRU(nn.Module):\n",
        "    def __init__(self, n_layers, hidden_dim, n_vocab, embed_dim, n_classes, dropout_p=0.2):\n",
        "        super(BasicGRU, self).__init__()\n",
        "        self.n_layers = n_layers \n",
        "\n",
        "        self.embed = nn.Embedding(n_vocab, embed_dim)\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.dropout = nn.Dropout(dropout_p)\n",
        "\n",
        "        self.gru = nn.GRU(embed_dim, self.hidden_dim,\n",
        "                          num_layers=self.n_layers,\n",
        "                          batch_first=True)\n",
        "        \n",
        "        self.out = nn.Linear(self.hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.embed(x)\n",
        "        x, _ = self.gru(x)  \n",
        "\n",
        "        h_t = x[:,-1,:]\n",
        "\n",
        "        self.dropout(h_t)\n",
        "\n",
        "        out = self.out(h_t) \n",
        "        return out\n",
        "\n",
        "model = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().to(device)    \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# train\n",
        "for epoch in range(training_epochs):\n",
        "  avg_cost = 0\n",
        "  for batch in train_iter:\n",
        "    X, Y = batch.text.to(device), batch.label.to(device)\n",
        "    Y.data.sub_(1)\n",
        "    optimizer.zero_grad()\n",
        "    hypothesis = model(X)\n",
        "    cost = criterion(hypothesis, Y)\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "    avg_cost += cost / batch_size\n",
        "  print('[Epoch: {:>4}] cost = {:>.9}'.format(epoch + 1, avg_cost))\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/MyDrive/인공지능응용/hw14_model_s1.pt')\n",
        "\n",
        "model_new = BasicGRU(1, 256, vocab_size, 128, n_classes, 0.5).to(device)\n",
        "model_new.load_state_dict(torch.load('/content/drive/MyDrive/인공지능응용/hw14_model_s1.pt'))\n",
        "\n",
        "corrects = 0\n",
        "for batch in val_iter:\n",
        "  x,y = batch.text.to(device), batch.label.to(device)\n",
        "  y.data.sub_(1)\n",
        "  hypothesis = model_new(x)\n",
        "  corrects += (hypothesis.max(1)[1].view(y.size()).data == y.data).sum() \n",
        "\n",
        "print('accuracy = ', corrects/len(val_iter.dataset)*100.0)\n",
        "\n",
        "corrects = 0\n",
        "for batch in test_iter:\n",
        "  x,y = batch.text.to(device), batch.label.to(device)\n",
        "  y.data.sub_(1)\n",
        "  hypothesis = model_new(x)\n",
        "  corrects += (hypothesis.max(1)[1].view(y.size()).data == y.data).sum() \n",
        "\n",
        "print('accuracy = ', corrects/len(val_iter.dataset)*100.0)\n",
        "\n",
        "input_text = testset[3].text\n",
        "print(input_text)\n",
        "for i in range(len(input_text)):\n",
        "  print(TEXT.vocab[input_text[i]], end = ', ')\n",
        "\n",
        "result = []\n",
        "for i in range(len(input_text)):\n",
        "  result.append(TEXT.vocab[input_text[i]])\n",
        "print(\" \", end='\\n')\n",
        "print('----------------------')\n",
        "\n",
        "result = torch.LongTensor(result)\n",
        "result = result.unsqueeze(0).to(device)\n",
        "\n",
        "hypothesis = model_new(result)\n",
        "print(hypothesis.max(1)[1])"
      ]
    }
  ]
}