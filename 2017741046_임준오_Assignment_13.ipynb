{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2017741046_임준오_Assignment_13.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYoQUCO4U_TJ"
      },
      "source": [
        "##**3 Assignment**\n",
        "\n",
        "다음 미완성 코드를 활용해 좀 더 긴 문장을 학습해보자\n",
        "\n",
        "* Sample sentences\n",
        "  * \"if you want to build a ship, don't drum up people together to \"\n",
        "  * \"collect wood and don't assign them tasks and work, but rather \"\n",
        "  * \"teach them to long for the endless immensity of the sea.\"\n",
        "* Training data sentence\n",
        "  * Shape: (N, S, E)\n",
        "* Hidden\n",
        "  * Shape: (N, S, E *2)\n",
        "* Output\n",
        "  * Shape: (N, S, E)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1x446iaNhspH",
        "outputId": "e7ac015e-9049-42b5-a837-6608ac7bf70a"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# for reproducibility\n",
        "torch.manual_seed(100)\n",
        "\n",
        "# Dictionary\n",
        "sample_sentence_1 = \"if you want to build a ship, don't drum up people together to \"\n",
        "sample_sentence_2 = \"collect wood and don't assign them tasks and work, but rather \"\n",
        "sample_sentence_3 = \"teach them to long for the endless immensity of the sea.\"\n",
        "sample_sentence = sample_sentence_1 + sample_sentence_2 + sample_sentence_3\n",
        "char_set = list(set(sample_sentence))\n",
        "dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "# Parameters\n",
        "dic_size = len(dic)\n",
        "input_size = dic_size\n",
        "hidden_size = dic_size * 2\n",
        "output_size = dic_size\n",
        "unit_sequence_length = 20\n",
        "\n",
        "# Dataset setting\n",
        "input_batch = []\n",
        "target_batch = []\n",
        "input_batch1 = []\n",
        "\n",
        "x_data = [dic[c] for c in sample_sentence[:-1]]\n",
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data]\n",
        "y_data = [dic[c] for c in sample_sentence[1:]]\n",
        "\n",
        "input_batch1.append(x_one_hot)\n",
        "X1 = torch.FloatTensor(np.array(input_batch))\n",
        "\n",
        "num = 0\n",
        "N = 0\n",
        "for _ in range(160):\n",
        "    mini_batch_x = []\n",
        "    mini_batch_y = []\n",
        "    num = N\n",
        "    for _ in range(unit_sequence_length):\n",
        "        mini_batch_x.append(x_one_hot[num])\n",
        "        mini_batch_y.append(y_data[num])\n",
        "        num += 1\n",
        "    N += 1\n",
        "    input_batch.append(mini_batch_x)\n",
        "    target_batch.append(mini_batch_y)\n",
        "\n",
        "X = torch.FloatTensor(np.array(input_batch))\n",
        "Y = torch.LongTensor(np.array(target_batch))\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "class Custom_RNN(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
        "    super(Custom_RNN, self).__init__()\n",
        "    self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=layers)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "class Custom_LSTM(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
        "    super(Custom_LSTM, self).__init__()\n",
        "    self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=layers)\n",
        "    self.fc = nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.lstm(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "class Custom_GRU(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
        "    super(Custom_GRU, self).__init__()\n",
        "    self.gru = torch.nn.GRU(input_dim, hidden_dim, num_layers=layers)\n",
        "    self.fc = torch.nn.Linear(hidden_dim, output_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.gru(x)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "learning_rate = 0.05\n",
        "training_epochs = 100\n",
        "model1 = Custom_RNN(input_size, hidden_size, output_size, 2)\n",
        "model2 = Custom_LSTM(input_size, hidden_size, output_size, 2)\n",
        "model3 = Custom_GRU(input_size, hidden_size, output_size, 2)\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "criterion = nn.CrossEntropyLoss()    # Softmax\n",
        "optimizer1 = optim.Adam(model1.parameters(), lr=learning_rate)\n",
        "optimizer2 = optim.Adam(model2.parameters(), lr=learning_rate)\n",
        "optimizer3 = optim.Adam(model3.parameters(), lr=learning_rate)\n",
        "\n",
        "# train\n",
        "for epoch in range(training_epochs):\n",
        "    optimizer1.zero_grad()\n",
        "    optimizer2.zero_grad()\n",
        "    optimizer3.zero_grad()\n",
        "\n",
        "    output1 = model1(X)\n",
        "    output2 = model2(X)\n",
        "    output3 = model3(X)\n",
        "\n",
        "    loss1 = criterion(output1.reshape(-1, dic_size), Y.reshape(-1))\n",
        "    loss2 = criterion(output2.reshape(-1, dic_size), Y.reshape(-1))\n",
        "    loss3 = criterion(output3.reshape(-1, dic_size), Y.reshape(-1))\n",
        "    loss1.backward()\n",
        "    loss2.backward()\n",
        "    loss3.backward()\n",
        "    optimizer1.step()\n",
        "    optimizer2.step()\n",
        "    optimizer3.step()\n",
        "\n",
        "    if epoch % 10 == 9:\n",
        "        print('RNN  ','epoch: ',epoch, 'loss: ', loss1.item())\n",
        "        print('LSTM  ','epoch: ',epoch, 'loss: ', loss2.item()) \n",
        "        print('GRU  ','epoch: ',epoch, 'loss: ', loss3.item())\n",
        "        print('------------------------------------------')\n",
        "\n",
        "# result\n",
        "result1 = output1.data.numpy().argmax(axis=2)\n",
        "result2 = output2.data.numpy().argmax(axis=2)\n",
        "result3 = output3.data.numpy().argmax(axis=2)\n",
        "\n",
        "result_sentence1 = []\n",
        "for i,  character in enumerate(result1):\n",
        "  result_sentence1.append(char_set[character[0]])\n",
        "  if i == len(result1) - 1:\n",
        "      for sen in character:\n",
        "        result_sentence1.append(char_set[sen])\n",
        "\n",
        "result_sentence2 = []\n",
        "for i,  character in enumerate(result2):\n",
        "  result_sentence2.append(char_set[character[0]])\n",
        "  if i == len(result2) - 1:\n",
        "      for sen in character:\n",
        "        result_sentence2.append(char_set[sen])\n",
        "        \n",
        "result_sentence3 = []\n",
        "for i,  character in enumerate(result3):\n",
        "  result_sentence3.append(char_set[character[0]])\n",
        "  if i == len(result3) - 1:\n",
        "      for sen in character:\n",
        "        result_sentence3.append(char_set[sen])     \n",
        "        \n",
        "result_sentence1 = ''.join(result_sentence1)\n",
        "result_sentence2 = ''.join(result_sentence2)\n",
        "result_sentence3 = ''.join(result_sentence3)   \n",
        "\n",
        "ratio1 = SequenceMatcher(None, result_sentence1, sample_sentence).ratio()\n",
        "ratio2 = SequenceMatcher(None, result_sentence2, sample_sentence).ratio()\n",
        "ratio3 = SequenceMatcher(None, result_sentence3, sample_sentence).ratio()\n",
        "\n",
        "print('RNN Result: ',result_sentence1)\n",
        "print('RNN Accuracy: ',ratio1)\n",
        "print('------------------------------------------')\n",
        "print('LSTM Result: ',result_sentence2)\n",
        "print('LSTM Accuracy: ',ratio2) \n",
        "print('------------------------------------------')\n",
        "print('GRU Result: ',result_sentence3)\n",
        "print('GRU Accuracy: ',ratio3)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([160, 20, 25])\n",
            "torch.Size([160, 20])\n",
            "RNN   epoch:  9 loss:  2.8800301551818848\n",
            "LSTM   epoch:  9 loss:  2.8676562309265137\n",
            "GRU   epoch:  9 loss:  2.1204166412353516\n",
            "------------------------------------------\n",
            "RNN   epoch:  19 loss:  2.810518741607666\n",
            "LSTM   epoch:  19 loss:  2.840773344039917\n",
            "GRU   epoch:  19 loss:  0.5478891730308533\n",
            "------------------------------------------\n",
            "RNN   epoch:  29 loss:  2.6897928714752197\n",
            "LSTM   epoch:  29 loss:  2.7778165340423584\n",
            "GRU   epoch:  29 loss:  0.0734465941786766\n",
            "------------------------------------------\n",
            "RNN   epoch:  39 loss:  2.6187591552734375\n",
            "LSTM   epoch:  39 loss:  2.6021804809570312\n",
            "GRU   epoch:  39 loss:  0.020794501528143883\n",
            "------------------------------------------\n",
            "RNN   epoch:  49 loss:  2.450483798980713\n",
            "LSTM   epoch:  49 loss:  2.3426361083984375\n",
            "GRU   epoch:  49 loss:  0.008466236293315887\n",
            "------------------------------------------\n",
            "RNN   epoch:  59 loss:  2.2634596824645996\n",
            "LSTM   epoch:  59 loss:  1.9280929565429688\n",
            "GRU   epoch:  59 loss:  0.005781099200248718\n",
            "------------------------------------------\n",
            "RNN   epoch:  69 loss:  2.0679361820220947\n",
            "LSTM   epoch:  69 loss:  1.3752179145812988\n",
            "GRU   epoch:  69 loss:  0.004942381288856268\n",
            "------------------------------------------\n",
            "RNN   epoch:  79 loss:  1.8795254230499268\n",
            "LSTM   epoch:  79 loss:  0.7018347978591919\n",
            "GRU   epoch:  79 loss:  0.004547311924397945\n",
            "------------------------------------------\n",
            "RNN   epoch:  89 loss:  1.7234869003295898\n",
            "LSTM   epoch:  89 loss:  0.2951291799545288\n",
            "GRU   epoch:  89 loss:  0.00434701144695282\n",
            "------------------------------------------\n",
            "RNN   epoch:  99 loss:  1.5973014831542969\n",
            "LSTM   epoch:  99 loss:  0.17970283329486847\n",
            "GRU   epoch:  99 loss:  0.004218474496155977\n",
            "------------------------------------------\n",
            "RNN result:  l won'ton',to lummd andhem  tonkt roim donlh l e to l  t eto pond  t tomd ans won't ans s  toem to  , tnd wonk, aum dm  e  to ch them to pon' wosktoeldnd e   dmmmt   t  tn toemdn  \n",
            "RNN Accuracy:  0.37222222222222223\n",
            "------------------------------------------\n",
            "LSTM result:  l you uont to cuild anship, don't drum up people together te collect wood and don't dssign them tosks and work, but rather teach them to long for the cndless immmensity to the cel.\n",
            "LSTM Accuracy:  0.9277777777777778\n",
            "------------------------------------------\n",
            "GRU result:  f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immmensity of the sea.\n",
            "GRU Accuracy:  0.9944444444444445\n"
          ]
        }
      ]
    }
  ]
}