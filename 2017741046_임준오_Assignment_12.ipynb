{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2017741046_임준오_Assignment_12.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jEJvgilNGBU"
      },
      "source": [
        "##**3 Assignment**\n",
        "\n",
        "**다음 3개의 문장을 batch data로 활용해 RNN을 학습해보자 (아래의 미완성 코드, 위 실습코드, 실행결과를 참고)**\n",
        "\n",
        "* 'howareyou'\n",
        "* 'whats up?'\n",
        "* 'iamgreat.'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaIFJ-ANM8yG",
        "outputId": "9e227741-038f-4de2-db4b-c7b718523059"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(100)\n",
        "\n",
        "sample_sentences = ['howareyou', 'whats up?', 'iamgreat.']\n",
        "char_set = list(set(''.join(sample_sentences)))\n",
        "dic = {c: i for i, c in enumerate(char_set)}\n",
        "\n",
        "dic_size = len(dic)\n",
        "input_size = dic_size\n",
        "hidden_size = dic_size\n",
        "\n",
        "input_batch = []\n",
        "target_batch = []\n",
        "\n",
        "for sentence in sample_sentences:\n",
        "    x_data = [dic[c] for c in sentence[:-1]] #howareyo 딕셔너리의 key값에 대응되는 value값으로 구성\n",
        "    x_one_hot = [np.eye(dic_size)[x] for x in x_data] \n",
        "    y_data = [dic[c] for c in sentence[1:]] #owareyou\n",
        "\n",
        "    input_batch.append(x_one_hot)\n",
        "    target_batch.append(y_data)\n",
        "\n",
        "X = torch.FloatTensor(input_batch)\n",
        "Y = torch.LongTensor(target_batch)\n",
        "\n",
        "learning_rate = 0.05\n",
        "training_epochs = 800\n",
        "model = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()  \n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    outputs, _status = model(X)\n",
        "    loss = criterion(outputs.reshape(-1,dic_size),Y.reshape(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if epoch % 100 == 99:\n",
        "        result = outputs.data.numpy().argmax(axis=2)\n",
        "        i = 0\n",
        "        for sentence in result:\n",
        "            result_str = ''.join([char_set[c] for c in sentence])\n",
        "            print('epoch: ',epoch + 1, ' loss: ', loss.item(), ' prediction: ', result_str, ' true Y: ', sample_sentences[i][1:])\n",
        "            i += 1\n",
        "        print('--------------------------------------------------------------------------------')\n",
        "\n",
        "result = outputs.data.numpy().argmax(axis=2) #열 벡터에 대해서 max값\n",
        "for sentence in result:\n",
        "  print(''.join([char_set[c] for c in sentence]))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  100  loss:  1.2142258882522583  prediction:  owareyou  true Y:  owareyou\n",
            "epoch:  100  loss:  1.2142258882522583  prediction:  hats up?  true Y:  hats up?\n",
            "epoch:  100  loss:  1.2142258882522583  prediction:  amgreats  true Y:  amgreat.\n",
            "--------------------------------------------------------------------------------\n",
            "epoch:  200  loss:  1.2124203443527222  prediction:  owareyou  true Y:  owareyou\n",
            "epoch:  200  loss:  1.2124203443527222  prediction:  hats up?  true Y:  hats up?\n",
            "epoch:  200  loss:  1.2124203443527222  prediction:  amgreats  true Y:  amgreat.\n",
            "--------------------------------------------------------------------------------\n",
            "epoch:  300  loss:  1.1959898471832275  prediction:  owareyou  true Y:  owareyou\n",
            "epoch:  300  loss:  1.1959898471832275  prediction:  hats up?  true Y:  hats up?\n",
            "epoch:  300  loss:  1.1959898471832275  prediction:  amgreat.  true Y:  amgreat.\n",
            "--------------------------------------------------------------------------------\n",
            "epoch:  400  loss:  1.3312249183654785  prediction:  owat.you  true Y:  owareyou\n",
            "epoch:  400  loss:  1.3312249183654785  prediction:  hata up?  true Y:  hats up?\n",
            "epoch:  400  loss:  1.3312249183654785  prediction:  amgreat.  true Y:  amgreat.\n",
            "--------------------------------------------------------------------------------\n",
            "epoch:  500  loss:  1.1946427822113037  prediction:  owateyou  true Y:  owareyou\n",
            "epoch:  500  loss:  1.1946427822113037  prediction:  hats up?  true Y:  hats up?\n",
            "epoch:  500  loss:  1.1946427822113037  prediction:  amgreyt.  true Y:  amgreat.\n",
            "--------------------------------------------------------------------------------\n",
            "epoch:  600  loss:  1.1905481815338135  prediction:  owateyou  true Y:  owareyou\n",
            "epoch:  600  loss:  1.1905481815338135  prediction:  hats up?  true Y:  hats up?\n",
            "epoch:  600  loss:  1.1905481815338135  prediction:  amgreyt.  true Y:  amgreat.\n",
            "--------------------------------------------------------------------------------\n",
            "epoch:  700  loss:  1.188929557800293  prediction:  owateyou  true Y:  owareyou\n",
            "epoch:  700  loss:  1.188929557800293  prediction:  hats up?  true Y:  hats up?\n",
            "epoch:  700  loss:  1.188929557800293  prediction:  amgreyt.  true Y:  amgreat.\n",
            "--------------------------------------------------------------------------------\n",
            "epoch:  800  loss:  1.1939116716384888  prediction:  owareyou  true Y:  owareyou\n",
            "epoch:  800  loss:  1.1939116716384888  prediction:  hats up?  true Y:  hats up?\n",
            "epoch:  800  loss:  1.1939116716384888  prediction:  amgreat.  true Y:  amgreat.\n",
            "--------------------------------------------------------------------------------\n",
            "owareyou\n",
            "hats up?\n",
            "amgreat.\n"
          ]
        }
      ]
    }
  ]
}